{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domino Flow Training and Evaluation Workflow\n",
    "\n",
    "This notebook demonstrates how to execute a Domino Flow for training multiple fraud detection models and registering the best performing model to the Domino Model Registry.\n",
    "\n",
    "## Overview\n",
    "- Execute Domino Flow to train AdaBoost, GaussianNB, and XGBoost classifiers\n",
    "- Monitor execution progress\n",
    "- Compare model performance using Experiment Manager\n",
    "- Register the best model to Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Execute Domino Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected working directory: /mnt\n",
      "\n",
      "============================================================\n",
      "STARTING DOMINO FLOW EXECUTION\n",
      "============================================================\n",
      "\n",
      "Attempting to execute workflow via pyflyte...\n",
      "Note: If you encounter CLI parameter conflicts, you can:\n",
      "  1. Execute the workflow from the Domino Flows UI instead\n",
      "  2. Run each trainer script as separate Domino Jobs\n",
      "  3. Use the workflow from a Domino Workspace with flytekit installed\n",
      "\n",
      "‚úÖ Flow execution started successfully!\n",
      "\n",
      "Output:\n",
      "Running Execution on Remote.\n",
      "\u001b[1;34mUpgrade to ydata-sdk\u001b[0m\n",
      "Improve your data and profiling with ydata-sdk, featuring data quality scoring, redundancy detection, outlier identification, text validation, and synthetic data generation.\n",
      "Register at https://ydata.ai/register\n",
      "Creating task using latest values. This is not recommended, as values not explicitly defined may change between subsequent executions of this task\n",
      "Retrieving default properties for job against project 68f2477cfe21ba6bf5ff485b\n",
      "Resolved job properties: DominoJobConfig(Command='python exercises/d_TrainingAndEvaluation/trainer_ada.py', Title=None, CommitId='fdd13ba9bf0418a1f71475bd354dfe7d2dd37f3c', MainRepoGitRef=None, HardwareTierId='small-k8s', EnvironmentId='68f155b8d528a221685f4439', EnvironmentRevisionSpec=EnvironmentRevisionSpecification(EnvironmentRevisionType=<EnvironmentRevisionType.SomeRevision: 'SomeRevision'>, EnvironmentRevisionId='68f520328e2cd3097a7e78ac'), ComputeClusterProperties=None, VolumeSizeGiB=10, DatasetSnapshots=[DatasetSnapshot(Id='68f24783fe21ba6bf5ff485f', Version=3, Name='Fraud-Detection-Workshop')], ExternalVolumeMountIds=[], cache_ignore_vars=None)\n",
      "Creating task using latest values. This is not recommended, as values not explicitly defined may change between subsequent executions of this task\n",
      "Retrieving default properties for job against project 68f2477cfe21ba6bf5ff485b\n",
      "Resolved job properties: DominoJobConfig(Command='python exercises/d_TrainingAndEvaluation/trainer_gnb.py', Title=None, CommitId='fdd13ba9bf0418a1f71475bd354dfe7d2dd37f3c', MainRepoGitRef=None, HardwareTierId='small-k8s', EnvironmentId='68f155b8d528a221685f4439', EnvironmentRevisionSpec=EnvironmentRevisionSpecification(EnvironmentRevisionType=<EnvironmentRevisionType.SomeRevision: 'SomeRevision'>, EnvironmentRevisionId='68f520328e2cd3097a7e78ac'), ComputeClusterProperties=None, VolumeSizeGiB=10, DatasetSnapshots=[DatasetSnapshot(Id='68f24783fe21ba6bf5ff485f', Version=3, Name='Fraud-Detection-Workshop')], ExternalVolumeMountIds=[], cache_ignore_vars=None)\n",
      "Creating task using latest values. This is not recommended, as values not explicitly defined may change between subsequent executions of this task\n",
      "Retrieving default properties for job against project 68f2477cfe21ba6bf5ff485b\n",
      "Resolved job properties: DominoJobConfig(Command='python exercises/d_TrainingAndEvaluation/trainer_xgb.py', Title=None, CommitId='fdd13ba9bf0418a1f71475bd354dfe7d2dd37f3c', MainRepoGitRef=None, HardwareTierId='small-k8s', EnvironmentId='68f155b8d528a221685f4439', EnvironmentRevisionSpec=EnvironmentRevisionSpecification(EnvironmentRevisionType=<EnvironmentRevisionType.SomeRevision: 'SomeRevision'>, EnvironmentRevisionId='68f520328e2cd3097a7e78ac'), ComputeClusterProperties=None, VolumeSizeGiB=10, DatasetSnapshots=[DatasetSnapshot(Id='68f24783fe21ba6bf5ff485f', Version=3, Name='Fraud-Detection-Workshop')], ExternalVolumeMountIds=[], cache_ignore_vars=None)\n",
      "Creating task using latest values. This is not recommended, as values not explicitly defined may change between subsequent executions of this task\n",
      "Retrieving default properties for job against project 68f2477cfe21ba6bf5ff485b\n",
      "Resolved job properties: DominoJobConfig(Command='python exercises/d_TrainingAndEvaluation/compare.py', Title=None, CommitId='fdd13ba9bf0418a1f71475bd354dfe7d2dd37f3c', MainRepoGitRef=None, HardwareTierId='small-k8s', EnvironmentId='68f155b8d528a221685f4439', EnvironmentRevisionSpec=EnvironmentRevisionSpecification(EnvironmentRevisionType=<EnvironmentRevisionType.SomeRevision: 'SomeRevision'>, EnvironmentRevisionId='68f520328e2cd3097a7e78ac'), ComputeClusterProperties=None, VolumeSizeGiB=10, DatasetSnapshots=[DatasetSnapshot(Id='68f24783fe21ba6bf5ff485f', Version=3, Name='Fraud-Detection-Workshop')], ExternalVolumeMountIds=[], cache_ignore_vars=None)\n",
      "\u001b[?25l\n",
      "\u001b[2K\u001b[33m0:00:00\u001b[0m Running execution on remote.\n",
      "\u001b[2K\u001b[33m0:00:00\u001b[0m Running execution on remote.\n",
      "[‚úî] Go to https://ews.domino-eval.com/flows/console/projects/68f2477cfe21ba6bf5ff485b/domains/development/executions/forcibly-active-doe-qts6 to see the execution in the console.\n",
      "    Go to https://ews.domino-eval.com/u/integration-test/Fraud-Detection-Workshop/flows/redirect/forcibly-active-doe-qts6 to see the execution in Domino.\n",
      "\n",
      "\u001b[2K\u001b[33m0:00:00\u001b[0m Running execution on remote.\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2K\n",
      "\n",
      "Flow will create experiments under: CC Fraud Classifier Training integration-test_lhLEzu3T\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Store the flow execution ID for later use\n",
    "flow_execution_id = None\n",
    "experiment_name = None\n",
    "\n",
    "# Detect working directory (file-based or git-based project)\n",
    "domino_working_dir = os.environ.get(\"DOMINO_WORKING_DIR\", \"/mnt\")\n",
    "print(f\"Detected working directory: {domino_working_dir}\")\n",
    "\n",
    "# Add project root to path\n",
    "if domino_working_dir not in sys.path:\n",
    "    sys.path.insert(0, domino_working_dir)\n",
    "\n",
    "# Execute the Domino Flow\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STARTING DOMINO FLOW EXECUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Alternative approach: Use flytectl or direct Domino API\n",
    "    # If pyflyte has issues, we can execute via Domino Jobs directly\n",
    "    \n",
    "    print(\"\\nAttempting to execute workflow via pyflyte...\")\n",
    "    print(\"Note: If you encounter CLI parameter conflicts, you can:\")\n",
    "    print(\"  1. Execute the workflow from the Domino Flows UI instead\")\n",
    "    print(\"  2. Run each trainer script as separate Domino Jobs\")\n",
    "    print(\"  3. Use the workflow from a Domino Workspace with flytekit installed\\n\")\n",
    "    \n",
    "    # Run the workflow using pyflyte with minimal parameters\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            \"pyflyte\",\n",
    "            \"run\",\n",
    "            \"--remote\",\n",
    "            f\"{domino_working_dir}/exercises/d_TrainingAndEvaluation/workflow.py\",\n",
    "            \"credit_card_fraud_detection_workflow\"\n",
    "        ],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=120\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Flow execution started successfully!\")\n",
    "        print(f\"\\nOutput:\\n{result.stdout}\")\n",
    "        \n",
    "        # Extract experiment name from the environment or generate it\n",
    "        try:\n",
    "            # Import the domino_short_id function to generate the same experiment name\n",
    "            from domino_short_id import domino_short_id\n",
    "            experiment_name = f\"CC Fraud Classifier Training {domino_short_id()}\"\n",
    "            print(f\"\\nFlow will create experiments under: {experiment_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not determine experiment name: {e}\")\n",
    "            experiment_name = \"CC Fraud Classifier Training\"\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Flow execution failed with return code {result.returncode}\")\n",
    "        print(f\"\\nStderr:\\n{result.stderr}\")\n",
    "        print(f\"\\nStdout:\\n{result.stdout}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"ALTERNATIVE: Execute workflow manually\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"\\nYou can execute the training tasks individually:\")\n",
    "        print(\"  1. Run: python exercises/d_TrainingAndEvaluation/trainer_ada.py\")\n",
    "        print(\"  2. Run: python exercises/d_TrainingAndEvaluation/trainer_xgb.py\")\n",
    "        print(\"  3. Run: python exercises/d_TrainingAndEvaluation/trainer_gnb.py\")\n",
    "        print(\"  4. Run: python exercises/d_TrainingAndEvaluation/compare.py\")\n",
    "        print(\"\\nOr use the Domino Flows UI to create and execute the workflow visually.\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"‚è±Ô∏è  Workflow execution timed out (2 min limit for initial submission)\")\n",
    "    print(\"   The flow may still be running in the background.\")\n",
    "    print(\"   Check the Domino Flows UI for execution status.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error executing flow: {e}\")\n",
    "    print(\"\\nNote: Domino Flows requires proper platform environment setup\")\n",
    "    print(\"If CLI execution fails, please use the Domino Flows UI interface.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Monitor Flow Execution in Domino UI\n",
    "\n",
    "### Instructions for Viewing Flow Progress:\n",
    "\n",
    "1. **Navigate to Flows Dashboard**\n",
    "   - In the Domino UI, click on \"Flows\" in the left navigation panel\n",
    "   - You should see your \"credit_card_fraud_detection_workflow\" listed\n",
    "\n",
    "2. **Monitor Execution Status**\n",
    "   - Click on the flow name to view the execution details\n",
    "   - Watch the DAG (Directed Acyclic Graph) as tasks complete\n",
    "   - Each task will show status: Running, Completed, or Failed\n",
    "\n",
    "3. **Task Execution Order**\n",
    "   - Three training tasks run in parallel:\n",
    "     - Train AdaBoost classifier\n",
    "     - Train GaussianNB classifier  \n",
    "     - Train XGBoost classifier\n",
    "   - After all training completes, the comparison task executes\n",
    "\n",
    "4. **View Task Logs**\n",
    "   - Click on individual tasks to view their execution logs\n",
    "   - Monitor progress and check for any errors\n",
    "\n",
    "### Expected Flow Duration: 10-15 minutes\n",
    "\n",
    "Wait for all tasks to complete before proceeding to the next step.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Access Experiment Manager and Compare Results\n",
    "\n",
    "### Instructions for Experiment Manager:\n",
    "\n",
    "1. **Open Experiment Manager**\n",
    "   - Click \"Experiment Manager\" in the left navigation panel\n",
    "   - You should see 3 new experiment runs from the flow execution\n",
    "\n",
    "2. **Compare Model Performance**\n",
    "   - Select all 3 runs (AdaBoost, GaussianNB, XGBoost)\n",
    "   - Click \"Compare\" button in the top toolbar\n",
    "\n",
    "3. **Analyze Metrics**\n",
    "   - Review key metrics: ROC-AUC, Precision, Recall, F1-Score\n",
    "   - Look for the model with the highest performance\n",
    "   - Expected best performer: XGBoost\n",
    "\n",
    "4. **Review Model Details**\n",
    "   - Click on the best performing model\n",
    "   - Review complete traceability: code, data, parameters, artifacts\n",
    "   - Check model artifacts and performance visualizations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Register Best Model to Model Registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Best Performing Model\n",
    "\n",
    "This cell searches the MLflow experiment for the best performing model based on accuracy. It will:\n",
    "- Search for runs with accuracy metrics in the flow's experiment\n",
    "- Sort by accuracy (highest first)\n",
    "- Display key metrics (ROC-AUC, Precision, Recall, F1-Score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Best Model to Model Registry\n",
    "\n",
    "This cell registers the best performing model to the Domino Model Registry with:\n",
    "- Comprehensive model card and description\n",
    "- Model specifications (training framework, size, memory requirements)\n",
    "- Tags for governance and discoverability\n",
    "- Dataset information and performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the Preprocessing Pipeline\n",
    "\n",
    "This cell registers the feature scaling/preprocessing pipeline from Exercise 3 as a separate model endpoint. This creates the feature transformation endpoint needed by the Streamlit app and deployment services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section demonstrates programmatic model registration\n",
    "# In practice, you would typically do this through the Domino UI as shown in the instructions\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tracking\n",
    "from datetime import datetime\n",
    "\n",
    "def get_best_experiment_run(target_experiment_name=None):\n",
    "    \"\"\"\n",
    "    Retrieve the best performing experiment run based on accuracy from the specified experiment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if target_experiment_name is None:\n",
    "            target_experiment_name = experiment_name\n",
    "            \n",
    "        if target_experiment_name is None:\n",
    "            print(\"No experiment name specified\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"Searching for best model in experiment: {target_experiment_name}\")\n",
    "        \n",
    "        # Get the specific experiment by name\n",
    "        try:\n",
    "            experiment = mlflow.get_experiment_by_name(target_experiment_name)\n",
    "            if experiment is None:\n",
    "                print(f\"Experiment '{target_experiment_name}' not found\")\n",
    "                # Fall back to searching all experiments\n",
    "                experiments = mlflow.search_experiments()\n",
    "                if not experiments:\n",
    "                    print(\"No experiments found\")\n",
    "                    return None\n",
    "                experiment_ids = [exp.experiment_id for exp in experiments]\n",
    "            else:\n",
    "                experiment_ids = [experiment.experiment_id]\n",
    "                print(f\"Found experiment: {experiment.name} (ID: {experiment.experiment_id})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding experiment: {e}\")\n",
    "            # Fall back to all experiments\n",
    "            experiments = mlflow.search_experiments()\n",
    "            experiment_ids = [exp.experiment_id for exp in experiments] if experiments else []\n",
    "        \n",
    "        if not experiment_ids:\n",
    "            print(\"No experiments available\")\n",
    "            return None\n",
    "        \n",
    "        # Search for runs with accuracy metrics, ordered by accuracy (best first)\n",
    "        runs = mlflow.search_runs(\n",
    "            experiment_ids=experiment_ids,\n",
    "            filter_string=\"metrics.accuracy > 0\",\n",
    "            order_by=[\"metrics.accuracy DESC\"],\n",
    "            max_results=10\n",
    "        )\n",
    "        \n",
    "        if runs.empty:\n",
    "            print(\"No runs found with accuracy metrics in the target experiment\")\n",
    "            return None\n",
    "        \n",
    "        # Get the best performing run\n",
    "        best_run = runs.iloc[0]\n",
    "        \n",
    "        print(f\"Best model found:\")\n",
    "        print(f\"  Run ID: {best_run['run_id']}\")\n",
    "        print(f\"  Experiment: {best_run.get('experiment_id', 'Unknown')}\")\n",
    "        print(f\"  Model: {best_run.get('tags.model_type', 'Unknown')}\")\n",
    "        print(f\"  Accuracy: {best_run['metrics.accuracy']:.4f}\")\n",
    "        print(f\"  ROC-AUC: {best_run.get('metrics.roc_auc', 'N/A')}\")\n",
    "        print(f\"  Precision: {best_run.get('metrics.precision_fraud', 'N/A')}\")\n",
    "        print(f\"  Recall: {best_run.get('metrics.recall_fraud', 'N/A')}\")\n",
    "        print(f\"  F1-Score: {best_run.get('metrics.f1_fraud', 'N/A')}\")\n",
    "        \n",
    "        return best_run\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving experiments: {e}\")\n",
    "        return None\n",
    "\n",
    "# Get the best performing model from the current flow's experiment\n",
    "best_run = get_best_experiment_run(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register All Three Classifiers\n",
    "\n",
    "This cell registers **all three models** trained in the Domino Flow to the Model Registry:\n",
    "- **CC Fraud ADA Classifier** - AdaBoost ensemble model\n",
    "- **CC Fraud XGBoost Classifier** - Gradient boosting model\n",
    "- **CC Fraud GaussianNB Classifier** - Naive Bayes probabilistic model\n",
    "\n",
    "Each model is registered with:\n",
    "- Descriptive name and performance metrics\n",
    "- Model type tags and specifications\n",
    "- Metadata for governance and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_model_to_registry(run_info, base_model_name=\"fraud_detection_classifier\"):\n",
    "    \"\"\"\n",
    "    Register the best model to Domino Model Registry with model card and specifications.\n",
    "    Model name is made unique by appending project name for cross-project compatibility.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if run_info is None:\n",
    "            print(\"No run information provided\")\n",
    "            return None\n",
    "        \n",
    "        # Create unique model name using project name\n",
    "        project_name = os.environ.get('DOMINO_PROJECT_NAME', 'unknown-project')\n",
    "        project_owner = os.environ.get('DOMINO_PROJECT_OWNER', 'unknown-user')\n",
    "        model_name = f\"{base_model_name}_{project_owner}_{project_name}\"\n",
    "        \n",
    "        print(f\"üìù Creating unique model name: {model_name}\")\n",
    "        print(f\"   Project: {project_name}\")\n",
    "        print(f\"   Owner: {project_owner}\")\n",
    "            \n",
    "        run_id = run_info['run_id']\n",
    "        \n",
    "        # Extract training framework to determine correct artifact path\n",
    "        training_framework = None\n",
    "        methods = [\n",
    "            ('tags.model_type', run_info.get('tags.model_type')),\n",
    "            ('tags.mlflow.runName', run_info.get('tags.mlflow.runName', '')),\n",
    "            ('tags.model', run_info.get('tags.model')),\n",
    "        ]\n",
    "        \n",
    "        for method, value in methods:\n",
    "            if value and training_framework is None:\n",
    "                value_lower = str(value).lower()\n",
    "                if 'xgb' in value_lower or 'xgboost' in value_lower:\n",
    "                    training_framework = 'XGBoost'\n",
    "                    break\n",
    "                elif 'ada' in value_lower or 'adaboost' in value_lower:\n",
    "                    training_framework = 'AdaBoost'\n",
    "                    break\n",
    "                elif 'gnb' in value_lower or 'gaussian' in value_lower or 'naive' in value_lower:\n",
    "                    training_framework = 'GaussianNB'\n",
    "                    break\n",
    "        \n",
    "        # Fallback if no detection worked\n",
    "        if training_framework is None:\n",
    "            training_framework = 'Unknown'\n",
    "            print(f\"‚ö†Ô∏è Could not detect training framework\")\n",
    "        \n",
    "        # Construct correct artifact path based on model name\n",
    "        model_artifact_name = run_info.get('tags.model', training_framework).lower().replace(' ', '_')\n",
    "        artifact_path = f\"{model_artifact_name}_model\"\n",
    "        model_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
    "        \n",
    "        print(f\"üîç Detected model type: {training_framework}\")\n",
    "        print(f\"üìÅ Using artifact path: {artifact_path}\")\n",
    "        \n",
    "        # Create model version with correct model URI\n",
    "        model_version = mlflow.register_model(\n",
    "            model_uri=model_uri,\n",
    "            name=model_name\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Model registered successfully:\")\n",
    "        print(f\"   Name: {model_version.name}\")\n",
    "        print(f\"   Version: {model_version.version}\")\n",
    "        print(f\"   Run ID: {run_id}\")\n",
    "        print(f\"   Model URI: {model_uri}\")\n",
    "        \n",
    "        # Check dataset information used by Flow scripts\n",
    "        dataset_info, data_files = check_flow_datasets()\n",
    "        \n",
    "        # Print first 3 data files found\n",
    "        if data_files:\n",
    "            print(f\"üìÅ Data files used: {', '.join(data_files[:3])}\")\n",
    "            if len(data_files) > 3:\n",
    "                print(f\"   ... and {len(data_files) - 3} more files\")\n",
    "        \n",
    "        # Update model with comprehensive card template and specifications\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        \n",
    "        # Set simple one-sentence description for model version\n",
    "        desc = f\"Best performing fraud detection classifier from {project_owner}/{project_name} based on accuracy from automated flow training\"\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=model_version.version,\n",
    "            description=desc\n",
    "        )\n",
    "        \n",
    "        # Set basic model tags using client\n",
    "        basic_tags = {\n",
    "            \"model_type\": training_framework,\n",
    "            \"use_case\": \"fraud_detection\",\n",
    "            \"training_data\": \"credit_card_transactions\",\n",
    "            \"performance_metric\": \"accuracy\",\n",
    "            \"deployment_ready\": \"true\",\n",
    "            \"project_name\": project_name,\n",
    "            \"project_owner\": project_owner\n",
    "        }\n",
    "        \n",
    "        for key, value in basic_tags.items():\n",
    "            client.set_model_version_tag(model_name, model_version.version, key, value)\n",
    "        \n",
    "        # Check if model card template exists and use it for registered model description\n",
    "        domino_working_dir = os.environ.get(\"DOMINO_WORKING_DIR\", \"/mnt\")\n",
    "        template_path = f\"{domino_working_dir}/exercises/d_TrainingAndEvaluation/Model_Registry_template.md\"\n",
    "        try:\n",
    "            with open(template_path, 'r') as file:\n",
    "                model_card_description = file.read()\n",
    "                client.update_registered_model(model_name, model_card_description)\n",
    "                print(f\"‚úÖ Updated model description from template: {template_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not load model card template: {e}\")\n",
    "            # Fallback to simple description\n",
    "            fallback_desc = f\"Fraud Detection Classifier from {project_owner}/{project_name} trained using Domino Flow with multiple algorithm comparison\"\n",
    "            client.update_registered_model(model_name, fallback_desc)\n",
    "        \n",
    "        # Estimate model artifacts size based on model type\n",
    "        size_estimates = {\n",
    "            'XGBoost': '15-25 MB',\n",
    "            'AdaBoost': '8-15 MB', \n",
    "            'GaussianNB': '< 5 MB'\n",
    "        }\n",
    "        model_size = size_estimates.get(training_framework, '10-20 MB (estimated)')\n",
    "        \n",
    "        # Estimate inference memory requirements based on model type\n",
    "        memory_estimates = {\n",
    "            'XGBoost': '512 MB - 1 GB (tree ensemble requires memory for all trees)',\n",
    "            'AdaBoost': '256 MB - 512 MB (smaller ensemble, moderate memory)',\n",
    "            'GaussianNB': '128 MB - 256 MB (simple probabilistic model, minimal memory)'\n",
    "        }\n",
    "        inference_memory = memory_estimates.get(training_framework, '256 MB - 512 MB (estimated)')\n",
    "        \n",
    "        # Set relevant model specifications\n",
    "        model_specs = {\n",
    "            \"mlflow.domino.specs.Training Framework\": training_framework,\n",
    "            \"mlflow.domino.specs.Model Artifacts Size\": model_size,\n",
    "            \"mlflow.domino.specs.Training Dataset Size\": dataset_info,\n",
    "            \"mlflow.domino.specs.Inference Memory\": inference_memory\n",
    "        }\n",
    "        \n",
    "        print(f\"üîß Setting model specs:\")\n",
    "        for key, value in model_specs.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # Apply all model specifications\n",
    "        for key, value in model_specs.items():\n",
    "            try:\n",
    "                client.set_registered_model_tag(model_name, key, value)\n",
    "                print(f\"‚úÖ Set spec: {key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to set spec {key}: {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ Model registration completed\")\n",
    "        print(f\"   Unique Model Name: {model_name}\")\n",
    "        print(f\"   Training Framework: {training_framework}\")\n",
    "        print(f\"   Model Size: {model_size}\")\n",
    "        print(f\"   Dataset Info: {dataset_info}\")\n",
    "        print(f\"   Inference Memory: {inference_memory}\")\n",
    "        \n",
    "        return model_version\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error registering model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def check_flow_datasets():\n",
    "    \"\"\"\n",
    "    Check dataset information used by Flow scripts - fast file-based check\n",
    "    Returns dataset info and list of data files found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import os\n",
    "        data_files_found = []\n",
    "        \n",
    "        # Primary dataset file used by all trainers\n",
    "        dataset_filename = 'transformed_cc_transactions.csv'\n",
    "        \n",
    "        # Detect project type and set appropriate paths\n",
    "        domino_working_dir = os.environ.get(\"DOMINO_WORKING_DIR\", \"/mnt\")\n",
    "        domino_project_name = os.environ.get(\"DOMINO_PROJECT_NAME\", \"Fraud-Detection-Workshop\")\n",
    "        \n",
    "        # Check multiple possible locations for datasets based on project type\n",
    "        if domino_working_dir == \"/mnt/code\":\n",
    "            # Git-based project\n",
    "            search_paths = [\n",
    "                f\"/mnt/data/{domino_project_name}/\",\n",
    "                \"/mnt/artifacts/\",\n",
    "                domino_working_dir\n",
    "            ]\n",
    "        else:\n",
    "            # File-based project\n",
    "            search_paths = [\n",
    "                f\"/domino/datasets/local/{domino_project_name}/\",\n",
    "                \"/mnt/\",\n",
    "                \"/tmp/\"\n",
    "            ]\n",
    "        for search_path in search_paths:\n",
    "            if os.path.exists(search_path):\n",
    "                try:\n",
    "                    files = [f for f in os.listdir(search_path) \n",
    "                           if f.endswith('.csv') and os.path.isfile(os.path.join(search_path, f))]\n",
    "                    for file in files:\n",
    "                        if file not in data_files_found:\n",
    "                            data_files_found.append(file)\n",
    "                except Exception:\n",
    "                    continue\n",
    "        \n",
    "        # Try to get info from the main dataset file\n",
    "        dataset_info = \"~500,000 credit card transactions (estimated)\"\n",
    "        for search_path in search_paths:\n",
    "            dataset_path = os.path.join(search_path, dataset_filename)\n",
    "            if os.path.exists(dataset_path):\n",
    "                try:\n",
    "                    file_size_mb = os.path.getsize(dataset_path) / (1024 * 1024)\n",
    "                    with open(dataset_path, 'r') as f:\n",
    "                        first_line = f.readline()\n",
    "                        line_count = 1\n",
    "                        for _ in range(100):\n",
    "                            if f.readline():\n",
    "                                line_count += 1\n",
    "                            else:\n",
    "                                break\n",
    "                        sample_size = f.tell()\n",
    "                        if sample_size > 0:\n",
    "                            estimated_rows = int((file_size_mb * 1024 * 1024) / sample_size * line_count)\n",
    "                        else:\n",
    "                            estimated_rows = line_count\n",
    "                    dataset_info = f\"{estimated_rows:,} transactions ({file_size_mb:.1f} MB)\"\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    dataset_info = f\"Dataset found ({file_size_mb:.1f} MB)\"\n",
    "                    break\n",
    "        \n",
    "        return dataset_info, data_files_found\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not check dataset info: {e}\")\n",
    "        return \"~500,000 credit card transactions (estimated)\", []\n",
    "\n",
    "# Register the best model\n",
    "if best_run is not None:\n",
    "    registered_model = register_model_to_registry(best_run)\n",
    "else:\n",
    "    print(\"No best run available for registration\")\n",
    "    print(\"Please use the Domino UI to manually register the model as described in the instructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_preprocessing_pipeline():\n",
    "    \"\"\"\n",
    "    Register the preprocessing pipeline from Exercise 3 as a separate model endpoint.\n",
    "    Model name is made unique by appending project/user info for cross-project compatibility.\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    # Create unique model name\n",
    "    project_name = os.environ.get('DOMINO_PROJECT_NAME', 'unknown-project')\n",
    "    project_owner = os.environ.get('DOMINO_PROJECT_OWNER', 'unknown-user')\n",
    "    model_name = f\"CC_Fraud_Feature_Scaling_{project_owner}_{project_name}\"\n",
    "    \n",
    "    print(f\"üìù Creating unique preprocessing model name: {model_name}\")\n",
    "    \n",
    "    # Add project root to path to import from Exercise 3\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "    \n",
    "    try:\n",
    "        # Import the preprocessing functions from Exercise 3\n",
    "        from exercises.c_DataEngineering.data_engineering import add_derived_features\n",
    "        \n",
    "        # Look for existing preprocessing experiments\n",
    "        preprocessing_experiments = mlflow.search_experiments(\n",
    "            filter_string=\"name LIKE '%Preprocessing%'\"\n",
    "        )\n",
    "        \n",
    "        if preprocessing_experiments:\n",
    "            # Get the most recent preprocessing experiment\n",
    "            latest_experiment = max(preprocessing_experiments, \n",
    "                                  key=lambda x: x.creation_time)\n",
    "            \n",
    "            print(f\"Found preprocessing experiment: {latest_experiment.name}\")\n",
    "            \n",
    "            # Search for preprocessing runs in this experiment\n",
    "            preprocessing_runs = mlflow.search_runs(\n",
    "                experiment_ids=[latest_experiment.experiment_id],\n",
    "                filter_string=\"tags.pipeline = 'preprocessing'\",\n",
    "                order_by=[\"start_time DESC\"],\n",
    "                max_results=1\n",
    "            )\n",
    "            \n",
    "            if not preprocessing_runs.empty:\n",
    "                best_preprocessing_run = preprocessing_runs.iloc[0]\n",
    "                run_id = best_preprocessing_run.run_id\n",
    "                \n",
    "                print(f\"Found preprocessing run: {run_id}\")\n",
    "                \n",
    "                # Register the preprocessing pipeline as a model\n",
    "                preprocessing_model_uri = f\"runs:/{run_id}/preprocessing_pipeline\"\n",
    "                \n",
    "                try:\n",
    "                    # Register the preprocessing pipeline\n",
    "                    registered_model = mlflow.register_model(\n",
    "                        model_uri=preprocessing_model_uri,\n",
    "                        name=model_name\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"‚úÖ Registered preprocessing pipeline as model: {model_name}\")\n",
    "                    print(f\"   Model Version: {registered_model.version}\")\n",
    "                    print(f\"   Project: {project_name}\")\n",
    "                    print(f\"   Owner: {project_owner}\")\n",
    "                    \n",
    "                    # Update model version with description and tags\n",
    "                    client = mlflow.tracking.MlflowClient()\n",
    "                    \n",
    "                    # Add description\n",
    "                    client.update_model_version(\n",
    "                        name=model_name,\n",
    "                        version=registered_model.version,\n",
    "                        description=f\"Feature scaling pipeline for fraud detection from {project_owner}/{project_name}\"\n",
    "                    )\n",
    "                    \n",
    "                    # Add tags\n",
    "                    client.set_model_version_tag(\n",
    "                        name=model_name,\n",
    "                        version=registered_model.version,\n",
    "                        key=\"stage\",\n",
    "                        value=\"staging\"\n",
    "                    )\n",
    "                    \n",
    "                    client.set_model_version_tag(\n",
    "                        name=model_name,\n",
    "                        version=registered_model.version,\n",
    "                        key=\"model_type\",\n",
    "                        value=\"preprocessing\"\n",
    "                    )\n",
    "                    \n",
    "                    client.set_model_version_tag(\n",
    "                        name=model_name,\n",
    "                        version=registered_model.version,\n",
    "                        key=\"project_name\",\n",
    "                        value=project_name\n",
    "                    )\n",
    "                    \n",
    "                    client.set_model_version_tag(\n",
    "                        name=model_name,\n",
    "                        version=registered_model.version,\n",
    "                        key=\"project_owner\",\n",
    "                        value=project_owner\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"‚úÖ Updated model version with description and tags\")\n",
    "                    \n",
    "                    return registered_model\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error registering preprocessing model: {e}\")\n",
    "                    return None\n",
    "            else:\n",
    "                print(\"‚ùå No preprocessing runs found with 'preprocessing' tag\")\n",
    "                return None\n",
    "        else:\n",
    "            print(\"‚ùå No preprocessing experiments found\")\n",
    "            print(\"   Please run Exercise 3 (Data Engineering) first to create the preprocessing pipeline\")\n",
    "            return None\n",
    "            \n",
    "    except ImportError as e:\n",
    "        print(f\"‚ùå Could not import from Exercise 3: {e}\")\n",
    "        print(\"   Please ensure Exercise 3 (Data Engineering) has been completed\")\n",
    "        return None\n",
    "\n",
    "# Register the preprocessing pipeline\n",
    "print(\"üîÑ Registering preprocessing pipeline as feature scaling endpoint...\")\n",
    "preprocessing_model = register_preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_all_flow_models(target_experiment_name=None):\n",
    "    \"\"\"\n",
    "    Register all three classifiers trained in the Domino Flow to the Model Registry.\n",
    "    Each model is registered with a descriptive name: 'CC_Fraud_{Classifier}_Classifier_{owner}_{project}'\n",
    "    Model names are made unique by appending project/user info for cross-project compatibility.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get project information for unique naming\n",
    "        project_name = os.environ.get('DOMINO_PROJECT_NAME', 'unknown-project')\n",
    "        project_owner = os.environ.get('DOMINO_PROJECT_OWNER', 'unknown-user')\n",
    "        \n",
    "        print(f\"üìù Creating unique model names for project: {project_owner}/{project_name}\")\n",
    "        \n",
    "        if target_experiment_name is None:\n",
    "            target_experiment_name = experiment_name\n",
    "            \n",
    "        if target_experiment_name is None:\n",
    "            print(\"No experiment name specified\")\n",
    "            return []\n",
    "            \n",
    "        print(f\"Searching for all models in experiment: {target_experiment_name}\")\n",
    "        \n",
    "        # Get the specific experiment by name\n",
    "        try:\n",
    "            experiment = mlflow.get_experiment_by_name(target_experiment_name)\n",
    "            if experiment is None:\n",
    "                print(f\"Experiment '{target_experiment_name}' not found\")\n",
    "                return []\n",
    "            experiment_ids = [experiment.experiment_id]\n",
    "            print(f\"Found experiment: {experiment.name} (ID: {experiment.experiment_id})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error finding experiment: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Search for all runs with accuracy metrics\n",
    "        runs = mlflow.search_runs(\n",
    "            experiment_ids=experiment_ids,\n",
    "            filter_string=\"metrics.accuracy > 0\",\n",
    "            order_by=[\"metrics.accuracy DESC\"],\n",
    "            max_results=50\n",
    "        )\n",
    "        \n",
    "        if runs.empty:\n",
    "            print(\"No runs found with accuracy metrics\")\n",
    "            return []\n",
    "        \n",
    "        # Define the three models with unique naming pattern\n",
    "        model_mapping = {\n",
    "            'AdaBoost': f'CC_Fraud_ADA_Classifier_{project_owner}_{project_name}',\n",
    "            'XGBoost': f'CC_Fraud_XGBoost_Classifier_{project_owner}_{project_name}',\n",
    "            'GaussianNB': f'CC_Fraud_GaussianNB_Classifier_{project_owner}_{project_name}'\n",
    "        }\n",
    "        \n",
    "        registered_models = []\n",
    "        \n",
    "        # Find and register each model type\n",
    "        for model_type, registered_name in model_mapping.items():\n",
    "            # Search for this specific model type\n",
    "            matching_run = None\n",
    "            for _, run in runs.iterrows():\n",
    "                # Check multiple possible tag locations for model type\n",
    "                run_model_type = (\n",
    "                    run.get('tags.model', '') or\n",
    "                    run.get('tags.model_type', '') or \n",
    "                    run.get('tags.mlflow.runName', '') or\n",
    "                    ''\n",
    "                )\n",
    "                \n",
    "                # Match model type (case insensitive)\n",
    "                if model_type.lower() in str(run_model_type).lower():\n",
    "                    matching_run = run\n",
    "                    break\n",
    "            \n",
    "            if matching_run is None:\n",
    "                print(f\"‚ö†Ô∏è  No run found for {model_type}\")\n",
    "                continue\n",
    "            \n",
    "            # Register this model\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Registering {model_type} model as: {registered_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            run_id = matching_run['run_id']\n",
    "            \n",
    "            # CRITICAL FIX: Use correct artifact path\n",
    "            # Models are logged as \"{model_name}_model\" in generic_trainer.py\n",
    "            # Get the actual model name from tags\n",
    "            model_tag_name = matching_run.get('tags.model', model_type)\n",
    "            artifact_path = f\"{model_tag_name.lower().replace(' ', '_')}_model\"\n",
    "            model_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
    "            \n",
    "            # Print model metrics and artifact path\n",
    "            print(f\"  Run ID: {run_id}\")\n",
    "            print(f\"  Artifact Path: {artifact_path}\")\n",
    "            print(f\"  Model URI: {model_uri}\")\n",
    "            print(f\"  Accuracy: {matching_run.get('metrics.accuracy', 'N/A'):.4f}\")\n",
    "            print(f\"  ROC-AUC: {matching_run.get('metrics.roc_auc', 'N/A')}\")\n",
    "            print(f\"  Precision: {matching_run.get('metrics.precision_fraud', 'N/A')}\")\n",
    "            print(f\"  Recall: {matching_run.get('metrics.recall_fraud', 'N/A')}\")\n",
    "            print(f\"  F1-Score: {matching_run.get('metrics.f1_fraud', 'N/A')}\")\n",
    "            \n",
    "            try:\n",
    "                # Register the model with correct URI and unique name\n",
    "                model_version = mlflow.register_model(\n",
    "                    model_uri=model_uri,\n",
    "                    name=registered_name\n",
    "                )\n",
    "                \n",
    "                print(f\"\\n‚úÖ Model registered successfully:\")\n",
    "                print(f\"   Name: {model_version.name}\")\n",
    "                print(f\"   Version: {model_version.version}\")\n",
    "                print(f\"   Project: {project_name}\")\n",
    "                print(f\"   Owner: {project_owner}\")\n",
    "                \n",
    "                # Add metadata and tags\n",
    "                client = mlflow.tracking.MlflowClient()\n",
    "                \n",
    "                # Set description\n",
    "                desc = f\"{model_type} fraud detection classifier from {project_owner}/{project_name} trained via Domino Flow with ROC-AUC: {matching_run.get('metrics.roc_auc', 'N/A')}\"\n",
    "                client.update_model_version(\n",
    "                    name=registered_name,\n",
    "                    version=model_version.version,\n",
    "                    description=desc\n",
    "                )\n",
    "                \n",
    "                # Set basic tags (including project info)\n",
    "                basic_tags = {\n",
    "                    \"model_type\": model_type,\n",
    "                    \"use_case\": \"fraud_detection\",\n",
    "                    \"training_data\": \"credit_card_transactions\",\n",
    "                    \"deployment_ready\": \"true\",\n",
    "                    \"training_method\": \"domino_flow\",\n",
    "                    \"project_name\": project_name,\n",
    "                    \"project_owner\": project_owner\n",
    "                }\n",
    "                \n",
    "                for key, value in basic_tags.items():\n",
    "                    client.set_model_version_tag(registered_name, model_version.version, key, value)\n",
    "                \n",
    "                # Set model specifications\n",
    "                size_estimates = {\n",
    "                    'XGBoost': '15-25 MB',\n",
    "                    'AdaBoost': '8-15 MB', \n",
    "                    'GaussianNB': '< 5 MB'\n",
    "                }\n",
    "                memory_estimates = {\n",
    "                    'XGBoost': '512 MB - 1 GB',\n",
    "                    'AdaBoost': '256 MB - 512 MB',\n",
    "                    'GaussianNB': '128 MB - 256 MB'\n",
    "                }\n",
    "                \n",
    "                model_specs = {\n",
    "                    \"mlflow.domino.specs.Training Framework\": model_type,\n",
    "                    \"mlflow.domino.specs.Training Dataset Size\": \"~500,000 credit card transactions\",\n",
    "                    \"mlflow.domino.specs.Model Artifacts Size\": size_estimates.get(model_type, '10-20 MB'),\n",
    "                    \"mlflow.domino.specs.Inference Memory\": memory_estimates.get(model_type, '256 MB - 512 MB')\n",
    "                }\n",
    "                \n",
    "                for key, value in model_specs.items():\n",
    "                    client.set_registered_model_tag(registered_name, key, value)\n",
    "                \n",
    "                print(f\"‚úÖ Added metadata and tags\")\n",
    "                \n",
    "                registered_models.append({\n",
    "                    'name': registered_name,\n",
    "                    'version': model_version.version,\n",
    "                    'model_type': model_type,\n",
    "                    'run_id': run_id,\n",
    "                    'artifact_path': artifact_path\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error registering {model_type} model: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"REGISTRATION SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Successfully registered {len(registered_models)} models:\")\n",
    "        for model in registered_models:\n",
    "            print(f\"  ‚Ä¢ {model['name']} (v{model['version']}) - {model['model_type']}\")\n",
    "            print(f\"    Artifact: {model['artifact_path']}\")\n",
    "        \n",
    "        return registered_models\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in registration process: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return []\n",
    "\n",
    "# Register all three models from the flow\n",
    "print(\"Registering all models from the Domino Flow...\")\n",
    "all_registered_models = register_all_flow_models(experiment_name)\n",
    "\n",
    "if not all_registered_models:\n",
    "    print(\"\\n‚ö†Ô∏è  No models were registered. Please check that:\")\n",
    "    print(\"   1. The Domino Flow has completed successfully\")\n",
    "    print(\"   2. All three training tasks generated MLflow runs\")\n",
    "    print(\"   3. The experiment name is correct\")\n",
    "    print(\"\\nYou can also register models manually through the Domino UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Model Registration Instructions\n",
    "\n",
    "### If programmatic registration is not available, follow these steps in the Domino UI:\n",
    "\n",
    "1. **Select Best Model in Experiment Manager**\n",
    "   - Choose the run with highest ROC-AUC (typically XGBoost)\n",
    "   - Click on the run to view details\n",
    "\n",
    "2. **Register Model**\n",
    "   - Click \"Register Model From Run\" in the upper right corner\n",
    "   - Enter model name: `fraud_detection_classifier_v1`\n",
    "\n",
    "3. **Add Model Metadata**\n",
    "   - Description: \"Production-ready fraud detection classifier\"\n",
    "   - Tags: Use the template from `Model_Registry_template.md`\n",
    "   - Model specifications: Define input/output schemas\n",
    "\n",
    "4. **Governance and Approval**\n",
    "   - Review governance requirements\n",
    "   - Submit for approval if required by your organization\n",
    "   - Set deployment stage (Staging/Production)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the complete Domino Flow workflow for:\n",
    "\n",
    "1. **Flow Execution**: Running parallel model training tasks\n",
    "2. **Monitoring**: Tracking progress through Domino UI\n",
    "3. **Comparison**: Evaluating model performance in Experiment Manager\n",
    "4. **Registration**: Adding the best model to Model Registry\n",
    "\n",
    "### Key Domino Concepts Utilized:\n",
    "- **Domino Flows**: Visual workflow orchestration for ML pipelines\n",
    "- **Experiment Manager**: Centralized tracking and comparison of model runs\n",
    "- **Model Registry**: Cataloging and governance for production models\n",
    "\n",
    "### Next Steps:\n",
    "- Model deployment via REST API endpoints\n",
    "- Integration with Streamlit application\n",
    "- Production monitoring and governance\n",
    "\n",
    "This completes the **Training and Evaluation** phase of the fraud detection workshop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
